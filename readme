Contraction theory enables constructing a policy-learning framework that makes the closed-loop system trajectories inherently convergent towards a unique trajectory. At the technical level, identifying the contraction metric, which is the distance metric with respect to which a robot’s trajectories exhibit contraction is often non-trivial. We proposed to jointly learn the control policy and its corresponding contraction metric while enforcing contraction. To achieve this, we learn the robot dynamical model from an offline data set consisting of the robot’s state and input trajectories. Using this learned dynamical model, we propose a data augmentation algorithm for learning contraction policies. We randomly generate samples in the state-space from the system data set and propagate them forward in time through the learned dynamics model to generate auxiliary sample trajectories. We then learn both the control policy and the contraction metric such that the distance between the demonstration trajectories from the data set and our generated auxiliary sample trajectories decreases over time.
